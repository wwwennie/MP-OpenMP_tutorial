{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Passing Interface  (MPI)\n",
    "\n",
    "## Main purpose:\n",
    "communicating between processes\n",
    "\n",
    "## Types of communication:\n",
    "- synchronization\n",
    "- movement of data\n",
    "<img src=\"https://computing.llnl.gov/tutorials/mpi/images/hybrid_mem.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General philosophy\n",
    "\n",
    "- Distribute or send/receive data between processes\n",
    "\n",
    "- Like a email server for CPUs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Example: sorting\n",
    "<div class=\"right\">\n",
    "<img src=\"http://img.sparknotes.com/figures/B/becc4efefde067dce51a326cca23c5f0/mergesort.gif\">\n",
    "</div>\n",
    "\n",
    "- Full list is $O(N \\log N)$\n",
    "\n",
    "- Partial lists are $O(\\frac{N}{2} \\log \\frac{N}{2})$\n",
    "\n",
    "- Final sorting is $O(N)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Considerations while using MPI\n",
    "\n",
    "## All parallelism is explicity.\n",
    "\n",
    "## The user must correctly identify and implement the parallelization.\n",
    "\n",
    "The MPI construct only provides a few algorithms for passing data.  Each MPI process holds onto a single number *myid*.  Parallalization is implemented around these different numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approach to the tutorial\n",
    "\n",
    "## Most basic usage senario\n",
    "\n",
    "1) Read some input files on one process and distribute the data  \n",
    "2) Compute $\\pi$ by calculating $y = \\sqrt{1-x^2}$ on many different points  \n",
    "3) Collect the result from each MPI process to obtain $\\int_{-1}^{1}\\,\\sqrt{1-x^2}\\,dx=\\pi/2$    \n",
    "\n",
    "<div class=\"right\" markdown='1'>\n",
    "<img src=\"http://cercs-ed.gatech.edu/sites/default/files/u24/integrating_circle.jpg\" style=\"width:400px\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. First ordered list item\n",
    "2. Another item\n",
    " * Unordered sub-list. \n",
    "1. Actual numbers don't matter, just that it's a number\n",
    " 1. Ordered sub-list\n",
    "4. And another item.\n",
    "\n",
    "  You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).\n",
    "\n",
    "⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅\n",
    "⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅\n",
    "⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)\n",
    "\n",
    "* Unordered list can use asterisks\n",
    "- Or minuses\n",
    "+ Or pluses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hello world\n",
    "\n",
    "```c\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "int main(int argc, char *argv[]) {\n",
    "    int numprocs, myid;\n",
    "    MPI_Init(&argc,&argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n",
    "    printf (\"Number of tasks= %d My rank= %d\\n\", numprocs, myid);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run the code on a KNOT \n",
    "\n",
    "### We will compile the code with mpicc and run the code with mpirun\n",
    "\n",
    "\n",
    "```sh\n",
    ">>> which mpirun\n",
    "/opt/intel/compilers_and_libraries_2016.1.150/linux/mpi/intel64/bin/mpirun\n",
    "```\n",
    "### Intel's version of mpirun is loaded by default\n",
    "\n",
    "### But OPENMPI is much easier to link\n",
    "\n",
    "```sh\n",
    "export PATH=/opt/openmpi/bin:$PATH\n",
    "```\n",
    "\n",
    "The libarary path must also be availible **even during run time**\n",
    "\n",
    "```sh\n",
    "export LD_LIBRARY_PATH=/opt/openmpi/lib:$LD_LIBRARY_PATH\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "### mpicc from OpenMPI has some very nice features that the INTEL version does not.\n",
    "\n",
    "```sh\n",
    "# Show the flags necessary to compile MPI C applications\n",
    "shell$ mpicc --showme:compile\n",
    " \n",
    "# Show the flags necessary to link MPI C applications\n",
    "shell$ mpicc --showme:link\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Makefile\n",
    "\n",
    "\n",
    "```\n",
    "[jshen@knot hello]$ cat Makefile \n",
    "CC=/opt/openmpi/bin/mpicc\n",
    "MPI_COMPILE_FLAGS = $(shell $(CC) --showme:compile)\n",
    "MPI_LINK_FLAGS = $(shell $(CC) --showme:link)\n",
    "\n",
    "hello: hello.c\n",
    "\t$(CC) $(MPI_COMPILE_FLAGS) hello.c $(MPI_LINK_FLAGS) -o hello.x\n",
    "    \n",
    "[jshen@knot hello]$ make \n",
    "/opt/openmpi/bin/mpicc -I/opt/openmpi-1.6.4/include -pthread hello.c -pthread -L/opt/openmpi-1.6.4/lib -lmpi -ldl -lm -Wl,--export-dynamic -lrt -lnsl -lutil -o hello.x\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```c\n",
    "MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n",
    "MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n",
    "if (myid == 0) {\n",
    "    printf(\"WORKING on NODE %d\\n\", myid);\n",
    "    FILE *ifp;\n",
    "    char param[20];\n",
    "    int val;\n",
    "    ifp = fopen(\"cal_pi.inp\", \"r\");\n",
    "    while (fscanf(ifp, \"%s %d\", param, &val) != EOF) {\n",
    "         printf(\"%stt\\n\", &param);\n",
    "        if ( strcmp(param,\"div_min\") == 0) {\n",
    "            printf(\"div_min %d\\n\", val);\n",
    "            div_min=val;\n",
    "        }\n",
    "        if ( strcmp(param,\"div_max\") == 0) {\n",
    "            printf(\"div_min %d\\n\", val);\n",
    "            div_max=val;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "MPI_Bcast(&div_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "MPI_Bcast(&div_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "MPI_Barrier( MPI_COMM_WORLD ) ;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```c\n",
    "for (n=div_min; n<div_max;n++) {\n",
    "    if (n == 0) break;\n",
    "\n",
    "    h   = 1.0 / (double) n;\n",
    "    sum = 0.0;\n",
    "    for (i = myid + 1; i <= n; i += numprocs) { //increment by numprocs\n",
    "        x = h * ((double)i - 0.5);\n",
    "        sum += 4.0 / (1.0 + x*x);\n",
    "    }\n",
    "\n",
    "    mypi = h * sum;\n",
    "\n",
    "    MPI_Barrier( MPI_COMM_WORLD ) ;\n",
    "    MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    if (myid == 0)\n",
    "        printf(\"(n=%d) pi is approximately %.16f, Error is %.16f\\n\",\n",
    "                n, pi, fabs(pi - PI25DT));\n",
    "}\n",
    "MPI_Finalize();\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI\tStandard\t:\thttp://www.mpi-forum.org/docs/docs.html \n",
    "## MPI\tForum\t:\thttp://www.mpi-forum.org/ \n",
    "\n",
    "## MPI\timplementations: \n",
    "\n",
    "– MPICH\t:\thttp://www.mpich.org \n",
    "\n",
    "– MVAPICH\t:\thttp://mvapich.cse.ohio-state.edu/ \n",
    "\n",
    "– Intel\tMPI:\thttp://software.intel.com/en-us/intel -mpi -library/ \n",
    "\n",
    "– Open\tMPI\t:\thttp://www.open-mpi.org/ \n",
    "\n",
    "– IBM\tMPI,\tCray MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- You will see some combination of these of the different computing cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
